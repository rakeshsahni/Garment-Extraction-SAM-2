{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Garment Extraction Pipeline using Segment Anything Model (SAM) and Segment Anything Model 2 (SAM 2)\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "This notebook implements a state-of-the-art image segmentation pipeline for accurately extracting garment areas from sample images. It utilizes both the Segment Anything Model (SAM) and its successor, SAM2, developed by Facebook AI Research. The pipeline is coupled with an interactive Gradio interface for user-friendly mask generation.\n",
        "\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Setup and Installation](#setup-dependencies)\n",
        "2. [Importing Required Libraries](#import-libraries)\n",
        "3. [Model Loading](#model-loading)\n",
        "4. [Core Functionality](#core-functionality)\n",
        "5. [Image Processing Pipeline](#image-processing-pipeline)\n",
        "6. [Gradio Interface](#gradio-interface)\n",
        "7. [Execution](#execution)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pBFlWTkC_pNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown ## 2. Setup and Installation <a name=\"setup-dependencies\"></a>\n",
        "# @markdown - First, we install the necessary dependencies and download the pre-trained models.\n",
        "\n",
        "!pip install -q gradio opencv-python matplotlib\n",
        "\n",
        "# @markdown **Install SAM and SAM2**\n",
        "!pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "!pip install -q 'git+https://github.com/facebookresearch/segment-anything-2.git'\n",
        "\n",
        "# @markdown **Download model weights**\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Ph6WplMG_uUS",
        "outputId": "921caa24-f11f-401a-e72e-d9e650fdfe61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m974.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m267.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FlYWNr7LC8P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown ## 3. Import Required Libraries <a name=\"import-libraries\"></a>\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "\n",
        "# @markdown - SAM imports\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "# @markdown - SAM2 imports\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "G9LaIpoxHqs9",
        "outputId": "0875c394-dac2-4cbf-e761-35b843e8639e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sam2/modeling/sam/transformer.py:23: UserWarning: Flash Attention is disabled as it requires a GPU with Ampere (8.0) CUDA capability.\n",
            "  OLD_GPU, USE_FLASH_ATTN, MATH_KERNEL_ON = get_sdpa_settings()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EuQDBQQvIF93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown ## 4. Model Loading Functions <a name=\"model-loading\"></a>\n",
        "# @markdown - We define functions to load both SAM and SAM2 models, allowing for flexibility in model selection.\n",
        "\n",
        "def load_sam_model(checkpoint=\"sam_vit_h_4b8939.pth\", model_type=\"vit_h\", device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Load the Segment Anything Model (SAM).\n",
        "\n",
        "    Args:\n",
        "    checkpoint (str): Path to the SAM checkpoint file.\n",
        "    model_type (str): Type of the SAM model (e.g., \"vit_h\", \"vit_l\", \"vit_b\").\n",
        "    device (str): Device to load the model on (\"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    SamPredictor: Loaded SAM predictor object.\n",
        "    \"\"\"\n",
        "    sam = sam_model_registry[model_type](checkpoint=checkpoint)\n",
        "    sam.to(device=device)\n",
        "    return SamPredictor(sam)\n",
        "\n",
        "def load_sam2_model(model_cfg=\"sam2_hiera_l.yaml\", sam2_checkpoint=\"sam2_hiera_large.pt\", device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Load the Segment Anything Model 2 (SAM2).\n",
        "\n",
        "    Args:\n",
        "    model_cfg (str): Path to the SAM2 model configuration file.\n",
        "    sam2_checkpoint (str): Path to the SAM2 checkpoint file.\n",
        "    device (str): Device to load the model on (\"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    SAM2ImagePredictor: Loaded SAM2 predictor object.\n",
        "    \"\"\"\n",
        "    sam2_model = build_sam2(config_file=model_cfg, ckpt_path=sam2_checkpoint, device=device)\n",
        "    return SAM2ImagePredictor(sam2_model)\n",
        "\n",
        "# @markdown **Initialize the SAM & SAM2 models**\n",
        "sam_predictor = load_sam_model()\n",
        "sam2_predictor = load_sam2_model()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dQ6QYXX3IpM-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RLKYbj6ODLcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown ## 5. Core Functionality <a name=\"core-functionality\"></a>\n",
        "# @markdown - Generate a mask for the selected region in the image.\n",
        "# @markdown - Apply the generated mask to the input image.\n",
        "\n",
        "def generate_mask(image, point_coords):\n",
        "    \"\"\"\n",
        "    Generate a mask for the selected region in the image.\n",
        "\n",
        "    Args:\n",
        "    image (numpy.ndarray): Input image.\n",
        "    point_coords (list): List of coordinates [x, y] selected by the user.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Generated binary mask.\n",
        "    \"\"\"\n",
        "    global active_predictor\n",
        "    active_predictor.set_image(image)\n",
        "    input_point = np.array([point_coords])\n",
        "    input_label = np.array([1])  # 1 indicates a foreground point\n",
        "\n",
        "    masks, _, _ = active_predictor.predict(\n",
        "        point_coords=input_point,\n",
        "        point_labels=input_label,\n",
        "        multimask_output=False,\n",
        "    )\n",
        "    return masks[0]\n",
        "\n",
        "def mask_image(image, mask):\n",
        "    \"\"\"\n",
        "    Apply the generated mask to the input image.\n",
        "\n",
        "    Args:\n",
        "    image (numpy.ndarray): Input image.\n",
        "    mask (numpy.ndarray): Binary mask.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Masked image.\n",
        "    \"\"\"\n",
        "    masked_image = image.copy()\n",
        "    masked_image[mask == 0] = [0, 0, 0]  # Set background to black\n",
        "    return masked_image.astype(np.uint8)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZjFmk3G2JD5B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AJGhHzeKJdJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown ## 6. Image Processing Pipeline <a name=\"image-processing-pipeline\"></a>\n",
        "# @markdown - Process the input image and generate a masked output.\n",
        "# @markdown - Ensure image is in the correct format (H, W, C) and uint8.\n",
        "# @markdown - Convert mask to RGB for display.\n",
        "\n",
        "def process_image(image, evt: gr.SelectData):\n",
        "    \"\"\"\n",
        "    Process the input image and generate a masked output.\n",
        "\n",
        "    Args:\n",
        "    image (numpy.ndarray): Input image.\n",
        "    evt (gr.SelectData): Event data containing selected coordinates.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Tuple containing the original image, masked image, and mask.\n",
        "    \"\"\"\n",
        "    if image is None:\n",
        "        return None, None, None\n",
        "\n",
        "    # Ensure image is in the correct format (H, W, C) and uint8\n",
        "    if len(image.shape) == 2:\n",
        "        image = np.stack([image] * 3, axis=-1)\n",
        "    elif image.shape[2] == 4:\n",
        "        image = image[:, :, :3]\n",
        "\n",
        "    image = (image * 255).astype(np.uint8) if image.dtype == np.float32 else image.astype(np.uint8)\n",
        "\n",
        "    point_coords = [evt.index[0], evt.index[1]]\n",
        "    mask = generate_mask(image, point_coords)\n",
        "    masked_image = mask_image(image, mask)\n",
        "\n",
        "    # Convert mask to RGB for display\n",
        "    mask_rgb = np.stack([mask] * 3, axis=-1).astype(np.uint8) * 255\n",
        "\n",
        "    return image, masked_image, mask_rgb"
      ],
      "metadata": {
        "cellView": "form",
        "id": "s0xUyY93Jchi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2RbSGZEXJyxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown ## 7. Gradio Interface <a name=\"gradio-interface\"></a>\n",
        "# @markdown - Create a Gradio interface for user-friendly mask generation.\n",
        "\n",
        "active_predictor = sam_predictor\n",
        "def toggle_model(choice):\n",
        "    global active_predictor\n",
        "    active_predictor = sam_predictor if choice == \"SAM\" else sam2_predictor\n",
        "    return f\"Active Model: {choice}\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Advanced Garment Extraction using SAM and SAM2\")\n",
        "    gr.Markdown(\"Upload an image, select a model, and click on a region to generate a mask.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        input_image = gr.Image(label=\"Input Image\", type=\"numpy\")\n",
        "        masked_output = gr.Image(label=\"Masked Output\")\n",
        "        mask_output = gr.Image(label=\"Generated Mask\")\n",
        "\n",
        "    model_choice = gr.Radio([\"SAM\", \"SAM2\"], label=\"Select Model\", value=\"SAM\")\n",
        "    model_status = gr.Textbox(label=\"Active Model\", value=\"Active Model: SAM\")\n",
        "\n",
        "    input_image.select(process_image, inputs=[input_image], outputs=[input_image, masked_output, mask_output])\n",
        "    model_choice.change(toggle_model, inputs=[model_choice], outputs=[model_status])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "avgqe5fdJygf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LRo1uCecJ_2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown ## 8. Launch the Application <a name=\"execution\"></a>\n",
        "# @markdown - Run the Gradio application to start the mask generation process.\n",
        "\n",
        "# Launch the Gradio application\n",
        "demo.launch(debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "cellView": "form",
        "id": "ckQT9SElKC5g",
        "outputId": "9f157372-0208-43b4-c5e7-113c33dcd181"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://345f8517668df6d33e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://345f8517668df6d33e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DkDpqkapKORE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown ## 9. Conclusion\n",
        "# @markdown - This advanced garment extraction pipeline leverages both SAM and SAM2 models, providing a flexible and powerful solution for image segmentation tasks. The interactive Gradio interface allows for easy model switching and real-time mask generation, making it an ideal tool for both research and practical applications in computer vision and fashion technology."
      ],
      "metadata": {
        "cellView": "form",
        "id": "EQKS67EnJ_zv"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}